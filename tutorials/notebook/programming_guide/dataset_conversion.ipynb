{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MindSpore数据格式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "用户可以将非标准的数据集和常用的数据集转换为MindSpore数据格式，即MindRecord，从而方便地加载到MindSpore中进行训练。同时，MindSpore在部分场景做了性能优化，使用MindRecord可以获得更好的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非标准数据集转换MindRecord\n",
    "\n",
    "下面主要介绍如何将CV类数据和NLP类数据转换为MindRecord，并通过`MindDataset`实现MindRecord文件的读取。\n",
    "\n",
    "### 转换CV类数据集\n",
    "\n",
    "本示例主要介绍用户如何将自己的CV类数据集转换成MindRecord，并使用`MindDataset`读取。\n",
    "\n",
    "本示例首先创建一个包含100条记录的MindRecord文件，其样本包含`file_name`（字符串）、\n",
    "`label`（整形）、 `data`（二进制）三个字段，然后使用`MindDataset`读取该MindRecord文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: {'data': array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8), 'file_name': array(b'33.jpg', dtype='|S6'), 'label': array(33, dtype=int32)}\n",
      "Got 100 samples\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import os\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "from PIL import Image\n",
    "\n",
    "mindrecord_filename = \"test.mindrecord\"\n",
    "\n",
    "if os.path.exists(mindrecord_filename):\n",
    "    os.remove(mindrecord_filename)\n",
    "    os.remove(mindrecord_filename + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=mindrecord_filename, shard_num=1)\n",
    "\n",
    "cv_schema = {\"file_name\": {\"type\": \"string\"}, \"label\": {\"type\": \"int32\"}, \"data\": {\"type\": \"bytes\"}}\n",
    "writer.add_schema(cv_schema, \"it is a cv dataset\")\n",
    "\n",
    "writer.add_index([\"file_name\", \"label\"])\n",
    "\n",
    "data = []\n",
    "for i in range(100):\n",
    "    i += 1\n",
    "\n",
    "    sample = {}\n",
    "    white_io = BytesIO()\n",
    "    Image.new('RGB', (i*10, i*10), (255, 255, 255)).save(white_io, 'JPEG')  \n",
    "    image_bytes = white_io.getvalue()\n",
    "    sample['file_name'] = str(i) + \".jpg\"\n",
    "    sample['label'] = i\n",
    "    sample['data'] = white_io.getvalue()  \n",
    "\n",
    "    data.append(sample)\n",
    "    if i % 10 == 0:\n",
    "        writer.write_raw_data(data)\n",
    "        data = []\n",
    "\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()\n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=mindrecord_filename)  \n",
    "decode_op = vision.Decode()\n",
    "data_set = data_set.map(operations=decode_op, input_columns=[\"data\"], num_parallel_workers=2)  \n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator(output_numpy=True):\n",
    "    print(\"sample: {}\".format(item))\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换NLP类数据集\n",
    "\n",
    "本示例主要介绍用户如何将自己的NLP类数据集转换成MindRecord，并使用`MindDataset`读取。为了方便展示，此处略去了将文本转换成字典序的预处理过程。\n",
    "\n",
    "本示例首先创建一个包含100条记录的MindRecord文件，其样本包含八个字段，均为整形数组，然后使用`MindDataset`读取该MindRecord文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: {'source_eos_ids': Tensor(shape=[6], dtype=Int64, value= [62, 63, 64, 65, 66, 67]), 'source_eos_mask': Tensor(shape=[9], dtype=Int64, value= [19, 20, 21, 22, 23, 24, 25, 26, 27]), 'source_sos_ids': Tensor(shape=[5], dtype=Int64, value= [57, 58, 59, 60, 61]), 'source_sos_mask': Tensor(shape=[7], dtype=Int64, value= [ 57, 114, 171, 228, 285, 342, 399]), 'target_eos_ids': Tensor(shape=[9], dtype=Int64, value= [39, 40, 41, 42, 43, 44, 45, 46, 47]), 'target_eos_mask': Tensor(shape=[4], dtype=Int64, value= [48, 49, 50, 51]), 'target_sos_ids': Tensor(shape=[5], dtype=Int64, value= [28, 29, 30, 31, 32]), 'target_sos_mask': Tensor(shape=[6], dtype=Int64, value= [33, 34, 35, 36, 37, 38])}\n",
      "sample: {'source_eos_ids': Tensor(shape=[6], dtype=Int64, value= [51, 52, 53, 54, 55, 56]), 'source_eos_mask': Tensor(shape=[9], dtype=Int64, value= [19, 20, 21, 22, 23, 24, 25, 26, 27]), 'source_sos_ids': Tensor(shape=[5], dtype=Int64, value= [46, 47, 48, 49, 50]), 'source_sos_mask': Tensor(shape=[7], dtype=Int64, value= [ 46,  92, 138, 184, 230, 276, 322]), 'target_eos_ids': Tensor(shape=[9], dtype=Int64, value= [39, 40, 41, 42, 43, 44, 45, 46, 47]), 'target_eos_mask': Tensor(shape=[4], dtype=Int64, value= [48, 49, 50, 51]), 'target_sos_ids': Tensor(shape=[5], dtype=Int64, value= [28, 29, 30, 31, 32]), 'target_sos_mask': Tensor(shape=[6], dtype=Int64, value= [33, 34, 35, 36, 37, 38])}\n",
      "Got 100 samples\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.mindrecord import FileWriter\n",
    "\n",
    "mindrecord_filename = \"test.mindrecord\"\n",
    "\n",
    "if os.path.exists(mindrecord_filename):\n",
    "    os.remove(mindrecord_filename)\n",
    "    os.remove(mindrecord_filename + \".db\")\n",
    "\n",
    "writer = FileWriter(file_name=mindrecord_filename, shard_num=1)\n",
    "\n",
    "nlp_schema = {\"source_sos_ids\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"source_sos_mask\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"source_eos_ids\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"source_eos_mask\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"target_sos_ids\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"target_sos_mask\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"target_eos_ids\": {\"type\": \"int64\", \"shape\": [-1]},\n",
    "            \"target_eos_mask\": {\"type\": \"int64\", \"shape\": [-1]}}\n",
    "writer.add_schema(nlp_schema, \"it is a preprocessed nlp dataset\")\n",
    "\n",
    "data = []\n",
    "for i in range(100):  \n",
    "    i += 1\n",
    "\n",
    "    sample = {\"source_sos_ids\": np.array([i, i+1, i+2, i+3, i+4], dtype=np.int64),\n",
    "            \"source_sos_mask\": np.array([i*1, i*2, i*3, i*4, i*5, i*6, i*7], dtype=np.int64),\n",
    "            \"source_eos_ids\": np.array([i+5, i+6, i+7, i+8, i+9, i+10], dtype=np.int64),\n",
    "            \"source_eos_mask\": np.array([19, 20, 21, 22, 23, 24, 25, 26, 27], dtype=np.int64),\n",
    "            \"target_sos_ids\": np.array([28, 29, 30, 31, 32], dtype=np.int64),\n",
    "            \"target_sos_mask\": np.array([33, 34, 35, 36, 37, 38], dtype=np.int64),\n",
    "            \"target_eos_ids\": np.array([39, 40, 41, 42, 43, 44, 45, 46, 47], dtype=np.int64),\n",
    "            \"target_eos_mask\": np.array([48, 49, 50, 51], dtype=np.int64)}\n",
    "\n",
    "    data.append(sample)\n",
    "    if i % 10 == 0:\n",
    "      writer.write_raw_data(data)\n",
    "      data = []\n",
    "\n",
    "if data:\n",
    "    writer.write_raw_data(data)\n",
    "\n",
    "writer.commit()  \n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=mindrecord_filename)  \n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator():\n",
    "    print(\"sample: {}\".format(item))\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用数据集转换MindRecord\n",
    "\n",
    "MindSpore提供转换常用数据集的工具类，能够将常用的数据集转换为MindRecord。常用数据集及其对应的工具类列表如下。\n",
    "\n",
    "| 数据集 | 格式转换工具类 |\n",
    "| :-------- | :------------ |\n",
    "| CIFAR-10 | Cifar10ToMR |\n",
    "| CIFAR-100 | Cifar100ToMR |\n",
    "| ImageNet | ImageNetToMR |\n",
    "| MNIST | MnistToMR |\n",
    "| TFRecord | TFRecordToMR |\n",
    "| CSV File | CsvToMR |\n",
    "\n",
    "更多数据集转换的详细说明可参见[API文档](https://www.mindspore.cn/doc/api_python/zh-CN/master/mindspore/mindspore.mindrecord.html)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换CIFAR-10数据集\n",
    "\n",
    "用户可以通过`Cifar10ToMR`类，将CIFAR-10原始数据转换为MindRecord，并使用`MindDataset`读取。\n",
    "\n",
    "1. 下载[CIFAR-10数据集](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)并解压，其目录结构如下所示。\n",
    "\n",
    "    ```text\n",
    "    └─cifar-10-batches-py\n",
    "        ├─batches.meta\n",
    "        ├─data_batch_1\n",
    "        ├─data_batch_2\n",
    "        ├─data_batch_3\n",
    "        ├─data_batch_4\n",
    "        ├─data_batch_5\n",
    "        ├─readme.html\n",
    "        └─test_batch\n",
    "    ```\n",
    "\n",
    "2. 导入数据集转换工具类`Cifar10ToMR`。\n",
    "\n",
    "    ```python\n",
    "    from mindspore.mindrecord import Cifar10ToMR\n",
    "    ```\n",
    "\n",
    "3. 创建`Cifar10ToMR`对象，调用`transform`接口，将CIFAR-10数据集转换为MindRecord。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSRStatus.SUCCESS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.mindrecord import Cifar10ToMR\n",
    "\n",
    "CIFAR10_DIR = \"./cifar-10-batches-py\"\n",
    "MINDRECORD_FILE = \"./cifar10.mindrecord\"\n",
    "cifar10_transformer = Cifar10ToMR(CIFAR10_DIR, MINDRECORD_FILE)\n",
    "cifar10_transformer.transform(['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **参数说明：**\n",
    "   * `CIFAR10_DIR`：CIFAR-10数据集的文件夹路径。  \n",
    "   * `MINDRECORD_FILE`：输出的MindRecord文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 通过`MindDataset`读取MindRecord。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 50000 samples\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=MINDRECORD_FILE)\n",
    "decode_op = vision.Decode()\n",
    "data_set = data_set.map(operations=decode_op, input_columns=[\"data\"], num_parallel_workers=2)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator(output_numpy=True):\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换ImageNet数据集\n",
    "\n",
    "用户可以通过`ImageNetToMR`类，将ImageNet原始数据（图片、标注）转换为MindRecord，并使用`MindDataset`读取。\n",
    "\n",
    "1. 下载[ImageNet数据集](http://image-net.org/download)，将所有图片存放在`images/`文件夹，用一个映射文件`labels_map.txt`记录图片和标签的对应关系。映射文件包含2列，分别为各类别图片目录和标签ID，用空格隔开，映射文件示例如下：\n",
    "\n",
    "    ```text\n",
    "    n01440760 0\n",
    "    n01443537 1\n",
    "    n01484850 2\n",
    "    n01491361 3\n",
    "    n01494475 4\n",
    "    n01496331 5\n",
    "    ```\n",
    "\n",
    "    文件目录结构如下所示：\n",
    "\n",
    "    ```text\n",
    "    ├─ labels_map.txt\n",
    "    └─ images\n",
    "        └─ ......\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 导入数据集转换工具类`ImageNetToMR`。\n",
    "\n",
    "    ```python\n",
    "    from mindspore.mindrecord import ImageNetToMR\n",
    "    ```\n",
    "3. 创建`ImageNetToMR`对象，调用`transform`接口，将数据集转换为MindRecord。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSRStatus.SUCCESS"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mindspore.mindrecord import ImageNetToMR\n",
    "\n",
    "IMAGENET_MAP_FILE = \"./labels_map.txt\"\n",
    "IMAGENET_IMAGE_DIR = \"./images/\"\n",
    "MINDRECORD_FILE = \"./imagenet.mindrecord\"\n",
    "PARTITION_NUMBER = 8\n",
    "imagenet_transformer = ImageNetToMR(IMAGENET_MAP_FILE, IMAGENET_IMAGE_DIR, MINDRECORD_FILE, PARTITION_NUMBER)\n",
    "imagenet_transformer.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   **参数说明：**\n",
    "   - `IMAGENET_MAP_FILE`：ImageNet数据集标签映射文件的路径。  \n",
    "   - `IMAGENET_IMAGE_DIR`：包含ImageNet所有图片的文件夹路径。  \n",
    "   - `MINDRECORD_FILE`：输出的MindRecord文件路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 通过`MindDataset`读取MindRecord。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['file_name', 'image', 'label']\n",
      "sample: {'file_name': array(b'./images/cat/10.jpg', dtype='|S19'), 'image': array([[[184, 207, 125],\n",
      "        [184, 207, 125],\n",
      "        [184, 207, 125],\n",
      "        ...,\n",
      "        [ 82, 124,  76],\n",
      "        [ 83, 125,  77],\n",
      "        [ 83, 125,  77]],\n",
      "        ...,\n",
      "        [ 82, 124,  76],\n",
      "        [ 83, 125,  77],\n",
      "        [ 84, 126,  78]],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8), 'label': array(1, dtype=int32)}\n",
      "Got 10 samples\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=MINDRECORD_FILE + \"0\")\n",
    "decode_op = vision.Decode()\n",
    "print(data_set.get_col_names())\n",
    "data_set = data_set.map(operations=decode_op, input_columns=[\"image\"], num_parallel_workers=2)\n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator(output_numpy=True):\n",
    "    print(\"sample: {}\".format(item))\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换CSV数据集\n",
    "\n",
    "本示例首先创建一个包含5条记录的CSV文件，然后通过`CsvToMR`工具类将CSV文件转换为MindRecord，并最终通过`MindDataset`将其读取出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: {'english': array(90.), 'id': array(1, dtype=int64), 'math': array(78.5), 'name': array(b'Lily', dtype='|S4')}\n",
      "sample: {'english': array(99.), 'id': array(4, dtype=int64), 'math': array(95.), 'name': array(b'Tom', dtype='|S3')}\n",
      "sample: {'english': array(85.2), 'id': array(2, dtype=int64), 'math': array(99.), 'name': array(b'Lucy', dtype='|S4')}\n",
      "sample: {'english': array(78.5), 'id': array(5, dtype=int64), 'math': array(85.), 'name': array(b'Jeff', dtype='|S4')}\n",
      "sample: {'english': array(71.), 'id': array(3, dtype=int64), 'math': array(65.), 'name': array(b'Mike', dtype='|S4')}\n",
      "Got 5 samples\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.mindrecord import CsvToMR\n",
    "\n",
    "CSV_FILE_NAME = \"test.csv\"\n",
    "MINDRECORD_FILE_NAME = \"test.mindrecord\"\n",
    "PARTITION_NUM = 1\n",
    "\n",
    "def generate_csv():\n",
    "    headers = [\"id\", \"name\", \"math\", \"english\"]\n",
    "    rows = [(1, \"Lily\", 78.5, 90),\n",
    "          (2, \"Lucy\", 99, 85.2),\n",
    "          (3, \"Mike\", 65, 71),\n",
    "          (4, \"Tom\", 95, 99),\n",
    "          (5, \"Jeff\", 85, 78.5)]\n",
    "    with open(CSV_FILE_NAME, 'w', encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "generate_csv()\n",
    "\n",
    "if os.path.exists(MINDRECORD_FILE_NAME):\n",
    "    os.remove(MINDRECORD_FILE_NAME)\n",
    "    os.remove(MINDRECORD_FILE_NAME + \".db\")\n",
    "\n",
    "csv_transformer = CsvToMR(CSV_FILE_NAME, MINDRECORD_FILE_NAME, partition_number=PARTITION_NUM)\n",
    "\n",
    "csv_transformer.transform()\n",
    "\n",
    "assert os.path.exists(MINDRECORD_FILE_NAME)\n",
    "assert os.path.exists(MINDRECORD_FILE_NAME + \".db\")\n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=MINDRECORD_FILE_NAME)  \n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator(output_numpy=True):\n",
    "    print(\"sample: {}\".format(item))\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换TFRecord数据集\n",
    "\n",
    "> 目前支持TensorFlow 1.13.0-rc1及以上版本。\n",
    "\n",
    "本示例首先通过TensorFlow创建一个TFRecord文件，然后通过`TFRecordToMR`工具类将TFRecord文件转换为MindRecord，最后通过`MindDataset`将其读取出来，并使用`Decode`算子对`image_bytes`字段进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 10 rows in tfrecord.\n",
      "sample: {'file_name': array(b'0000.jpg', dtype='|S8'), 'float_list': array([0.000000e+00, 1.000000e+00, 2.800000e+00, 3.200000e+00,\n",
      "       4.400000e+00, 1.234569e+05, 9.876543e+07], dtype=float32), 'float_scalar': array(0., dtype=float32), 'image_bytes': array([[[255, 255, 255],\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8), 'int64_list': array([         8,          9,         10,         11,         12,\n",
      "       1234567898], dtype=int64), 'int64_scalar': array(8, dtype=int64)}\n",
      "Got 10 samples\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from io import BytesIO\n",
    "import os\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.mindrecord import TFRecordToMR\n",
    "import mindspore.dataset.vision.c_transforms as vision\n",
    "from PIL import Image\n",
    "import tensorflow as tf  \n",
    "\n",
    "\n",
    "TFRECORD_FILE_NAME = \"test.tfrecord\"\n",
    "MINDRECORD_FILE_NAME = \"test.mindrecord\"\n",
    "PARTITION_NUM = 1\n",
    "\n",
    "def generate_tfrecord():\n",
    "    def create_int_feature(values):\n",
    "        if isinstance(values, list):\n",
    "            feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))  \n",
    "        else:\n",
    "            feature = tf.train.Feature(int64_list=tf.train.Int64List(value=[values]))\n",
    "        return feature\n",
    "\n",
    "    def create_float_feature(values):\n",
    "        if isinstance(values, list):\n",
    "            feature = tf.train.Feature(float_list=tf.train.FloatList(value=list(values)))  \n",
    "        else:\n",
    "            feature = tf.train.Feature(float_list=tf.train.FloatList(value=[values]))\n",
    "        return feature\n",
    "\n",
    "    def create_bytes_feature(values):\n",
    "        if isinstance(values, bytes):\n",
    "            white_io = BytesIO()\n",
    "            Image.new('RGB', (10, 10), (255, 255, 255)).save(white_io, 'JPEG')\n",
    "            image_bytes = white_io.getvalue()\n",
    "            feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes]))\n",
    "        else:\n",
    "            feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=[bytes(values, encoding='utf-8')]))\n",
    "        return feature\n",
    "\n",
    "    writer = tf.io.TFRecordWriter(TFRECORD_FILE_NAME)\n",
    "\n",
    "    example_count = 0\n",
    "    for i in range(10):\n",
    "        file_name = \"000\" + str(i) + \".jpg\"\n",
    "        image_bytes = bytes(str(\"aaaabbbbcccc\" + str(i)), encoding=\"utf-8\")\n",
    "        int64_scalar = i\n",
    "        float_scalar = float(i)\n",
    "        int64_list = [i, i+1, i+2, i+3, i+4, i+1234567890]\n",
    "        float_list = [float(i), float(i+1), float(i+2.8), float(i+3.2),\n",
    "                    float(i+4.4), float(i+123456.9), float(i+98765432.1)]\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"file_name\"] = create_bytes_feature(file_name)\n",
    "        features[\"image_bytes\"] = create_bytes_feature(image_bytes)\n",
    "        features[\"int64_scalar\"] = create_int_feature(int64_scalar)\n",
    "        features[\"float_scalar\"] = create_float_feature(float_scalar)\n",
    "        features[\"int64_list\"] = create_int_feature(int64_list)\n",
    "        features[\"float_list\"] = create_float_feature(float_list)\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "        example_count += 1\n",
    "    writer.close()\n",
    "    print(\"Write {} rows in tfrecord.\".format(example_count))\n",
    "\n",
    "generate_tfrecord()\n",
    "\n",
    "feature_dict = {\"file_name\": tf.io.FixedLenFeature([], tf.string),\n",
    "              \"image_bytes\": tf.io.FixedLenFeature([], tf.string),\n",
    "              \"int64_scalar\": tf.io.FixedLenFeature([], tf.int64),\n",
    "              \"float_scalar\": tf.io.FixedLenFeature([], tf.float32),\n",
    "              \"int64_list\": tf.io.FixedLenFeature([6], tf.int64),\n",
    "              \"float_list\": tf.io.FixedLenFeature([7], tf.float32),\n",
    "              }\n",
    "\n",
    "if os.path.exists(MINDRECORD_FILE_NAME):\n",
    "    os.remove(MINDRECORD_FILE_NAME)\n",
    "    os.remove(MINDRECORD_FILE_NAME + \".db\")\n",
    "\n",
    "tfrecord_transformer = TFRecordToMR(TFRECORD_FILE_NAME, MINDRECORD_FILE_NAME, feature_dict, [\"image_bytes\"])\n",
    "tfrecord_transformer.transform()\n",
    "\n",
    "assert os.path.exists(MINDRECORD_FILE_NAME)\n",
    "assert os.path.exists(MINDRECORD_FILE_NAME + \".db\")\n",
    "\n",
    "data_set = ds.MindDataset(dataset_file=MINDRECORD_FILE_NAME)  \n",
    "decode_op = vision.Decode()\n",
    "data_set = data_set.map(operations=decode_op, input_columns=[\"image_bytes\"], num_parallel_workers=2)  \n",
    "count = 0\n",
    "for item in data_set.create_dict_iterator(output_numpy=True):\n",
    "    print(\"sample: {}\".format(item))\n",
    "    count += 1\n",
    "print(\"Got {} samples\".format(count))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
